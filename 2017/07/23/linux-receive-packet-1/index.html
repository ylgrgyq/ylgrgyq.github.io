<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Linux 网络协议栈收消息过程-Ring Buffer | A Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-80224793-1','auto');ga('send','pageview');</script></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Linux 网络协议栈收消息过程-Ring Buffer</h1><a id="logo" href="/.">A Blog</a><p class="description">by ylgrgyq</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Linux 网络协议栈收消息过程-Ring Buffer</h1><div class="post-meta">Jul 23, 2017<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><p>想看能不能完整梳理一下收消息过程。从 NIC 收数据开始，到触发软中断，交付数据包到 IP 层再经由路由机制到 TCP 层，最终交付用户进程。会尽力介绍收消息过程中的各种配置信息，以及各种监控数据。知道了收消息的完整过程，了解了各种配置，明白了各种监控数据后才有可能在今后的工作中做优化配置。</p>
<p>所有参考内容会列在这个系列最后一篇文章中。</p>
<p>Ring Buffer 相关的收消息过程大致如下：</p>
<img src="/2017/07/23/linux-receive-packet-1/ring-buffer.png" alt="图片来自参考1，对 raise softirq 的函数名做了修改，改为了 napi_schedule" title="图片来自参考1，对 raise softirq 的函数名做了修改，改为了 napi_schedule">
<p>NIC (network interface card) 在系统启动过程中会向系统注册自己的各种信息，系统会分配 Ring Buffer 队列也会分配一块专门的内核内存区域给 NIC 用于存放传输上来的数据包。struct sk_buff 是专门存放各种网络传输数据包的内存接口，在收到数据存放到 NIC 专用内核内存区域后，<a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/skbuff.h#L706" target="_blank" rel="external">sk_buff 内有个 data 指针会指向这块内存</a>。Ring Buffer 队列内存放的是一个个 Packet Descriptor ，其有两种状态： ready 和 used 。初始时 Descriptor 是空的，指向一个空的 sk_buff，处在 ready 状态。当有数据时，<a href="https://en.wikipedia.org/wiki/DMA" target="_blank" rel="external">DMA </a> 负责从 NIC 取数据，并在 Ring Buffer 上按顺序找到下一个 ready 的 Descriptor，将数据存入该 Descriptor 指向的 sk_buff 中，并标记槽为 used。因为是按顺序找 ready 的槽，所以 Ring Buffer 是个 FIFO 的队列。</p>
<p>当 DMA 读完数据之后，NIC 会触发一个 IRQ 让 CPU 去处理收到的数据。因为每次触发 IRQ 后 CPU 都要花费时间去处理 Interrupt Handler，如果 NIC 每收到一个 Packet 都触发一个 IRQ 会导致 CPU 花费大量的时间在处理 Interrupt Handler，处理完后又只能从 Ring Buffer 中拿出一个 Packet，虽然 Interrupt Handler 执行时间很短，但这么做也非常低效，并会给 CPU 带去很多负担。所以目前都是采用一个叫做 <a href="https://wiki.linuxfoundation.org/networking/napi" target="_blank" rel="external">New API(NAPI)</a> 的机制，去对 IRQ 做合并以减少 IRQ 次数。 </p>
<p>接下来介绍一下 NAPI 是怎么做到 IRQ 合并的。它主要是让 NIC 的 driver 能注册一个 <code>poll</code> 函数，之后 NAPI 的 subsystem 能通过 <code>poll</code> 函数去从 Ring Buffer 中批量拉取收到的数据。主要事件及其顺序如下：</p>
<ol>
<li>NIC driver 初始化时向 Kernel 注册 <code>poll</code> 函数，用于后续从 Ring Buffer 拉取收到的数据</li>
<li>driver 注册开启 NAPI，这个机制默认是关闭的，只有支持 NAPI 的 driver 才会去开启</li>
<li>收到数据后 NIC 通过 DMA 将数据存到内存</li>
<li>NIC 触发一个 IRQ，并触发 CPU 开始执行 driver 注册的 Interrupt Handler</li>
<li>driver 的 Interrupt Handler 通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/include/linux/netdevice.h#L421" target="_blank" rel="external">napi_schedule</a> 函数触发 softirq (<a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L3204" target="_blank" rel="external">NET_RX_SOFTIRQ</a>) 来唤醒 NAPI subsystem，NET_RX_SOFTIRQ 的 handler 是 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4861" target="_blank" rel="external">net_rx_action 会在另一个线程中被执行，在其中会调用 driver 注册的 <code>poll</code> 函数获取收到的 Packet</a></li>
<li>driver 会禁用当前 NIC 的 IRQ，从而能在 <code>poll</code> 完所有数据之前不会再有新的 IRQ</li>
<li>当所有事情做完之后，NAPI subsystem 会被禁用，并且会重新启用 NIC 的 IRQ</li>
<li>回到第三步</li>
</ol>
<p>从上面的描述可以看出来还缺一些东西，Ring Buffer 上的数据被 <code>poll</code> 走之后是怎么交付上层网络栈继续处理的呢？以及被消耗掉的 sk_buff 是怎么被重新分配重新放入 Ring Buffer 的呢？</p>
<p>这两个工作都在 <code>poll</code> 中完成，上面说过 <code>poll</code> 是个 driver 实现的函数，所以每个 driver 实现可能都不相同。但 <code>poll</code> 的工作基本是一致的就是：</p>
<ol>
<li>从 Ring Buffer 中将收到的 sk_buff 读取出来</li>
<li>对 sk_buff 做一些基本检查，可能会涉及到将几个 sk_buff 合并因为可能同一个 Frame 被分散放在多个 sk_buff 中</li>
<li>将 sk_buff 交付上层网络栈处理</li>
<li>清理 sk_buff，清理 Ring Buffer 上的 Descriptor 将其指向新分配的 sk_buff 并将状态设置为 ready</li>
<li>更新一些统计数据，比如收到了多少 packet，一共多少字节等</li>
</ol>
<p>如果拿 intel igb 这个网卡的实现来看，其 <code>poll</code> 函数在这里：<a href="http://elixir.free-electrons.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6361" target="_blank" rel="external">linux/drivers/net/ethernet/intel/igb/igb_main.c - Elixir - Free Electrons</a></p>
<p>首先是看到有 tx.ring 和 rx.ring，说明收发消息都会走到这里。发消息先不管，先看收消息，收消息走的是 <a href="http://elixir.free-electrons.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6912" target="_blank" rel="external">igb_clean_rx_irq</a>。收完消息后执行 <code>napi_complete_done</code> 退出 polling 模式，并开启 NIC 的 IRQ。从而我们知道大部分工作是在 igb_clean_rx_irq 中完成的，其实现大致上还是比较清晰的，就是上面描述的几步。里面有个 while 循环通过 buget 控制，从而在 Packet 特别多的时候不要让 CPU 在这里无穷循环下去，要让别的事情也能够被执行。循环内做的事情如下：</p>
<ol>
<li>先批量清理已经读出来的 sk_buff 并分配新的 buffer 从而避免每次读一个 sk_buff 就清理一个，很低效</li>
<li>找到 Ring Buffer 上下一个需要被读取的 Descriptor ，并检查描述符状态是否正常</li>
<li>根据 Descriptor 找到 sk_buff 读出来</li>
<li>检查是否是 End of packet，是的话说明 sk_buff 内有 Frame 的全部内容，不是的话说明 Frame 数据比 sk_buff 大，需要再读一个 sk_buff，将两个 sk_buff 数据合并起来</li>
<li>通过 Frame 的 Header 检查 Frame 数据完整性，是否正确之类的</li>
<li>记录 sk_buff 的长度，读了多少数据</li>
<li>设置 Hash、checksum、timestamp、VLAN id 等信息，这些信息是硬件提供的。</li>
<li>通过 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4354" target="_blank" rel="external">napi_gro_receive</a> 将 sk_buff 交付上层网络栈</li>
<li>更新一堆统计数据</li>
<li>回到 1，如果没数据或者 budget 不够就退出循环</li>
</ol>
<p>看到 budget 会影响到 CPU 执行 <code>poll</code> 的时间，budget 越大当数据包特别多的时候可以提高 CPU 利用率并减少数据包的延迟。但是 CPU 时间都花在这里会影响别的任务的执行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">budget 默认 300，可以调整</div><div class="line">sysctl -w net.core.netdev_budget=600</div></pre></td></tr></table></figure>
<p><code>napi_gro_receive</code>会涉及到 GRO 机制，稍后再说，大致上就是会对多个数据包做聚合，<code>napi_gro_receive</code> 最终是将处理好的 sk_buff 通过调用 <a href="http://elixir.free-electrons.com/linux/v4.4/source/net/core/dev.c#L4026" target="_blank" rel="external">netif_receive_skb</a>，将数据包送至上层网络栈。执行完 GRO 之后，基本可以认为数据包正式离开 Ring Buffer，进入下一个阶段了。在记录下一阶段的处理之前，补充一下收消息阶段 Ring Buffer 相关的更多细节。</p>
<h2 id="Generic-Receive-Offloading-GRO"><a href="#Generic-Receive-Offloading-GRO" class="headerlink" title="Generic Receive Offloading(GRO)"></a>Generic Receive Offloading(GRO)</h2><p>GRO 是 <a href="https://en.wikipedia.org/wiki/Large_receive_offload" target="_blank" rel="external">Large receive offload</a> 的一个实现。网络上大部分 MTU 都是 1500 字节，开启 Jumbo Frame 后能到 9000 字节，如果发送的数据超过 MTU 就需要切割成多个数据包。LRO 就是在收到多个数据包的时候将同一个 Flow 的多个数据包按照一定的规则合并起来交给上层处理，这样就能减少上层需要处理的数据包数量。</p>
<p>很多 LRO 机制是在 NIC 上实现的，没有实现 LRO 的 NIC 就少了上述合并数据包的能力。而 GRO 是 LRO 在软件上的实现，从而能让所有 NIC 都支持这个功能。</p>
<p><code>napi_gro_receive</code> 就是在收到数据包的时候合并多个数据包用的，如果收到的数据包需要被合并，<code>napi_gro_receive</code> 会很快返回。当合并完成后会调用 <code>napi_skb_finish</code> ，将因为数据包合并而不再用到的数据结构释放。最终会调用到 <code>netif_receive_skb</code> 将数据包交到上层网络栈继续处理。<code>netif_receive_skb</code> 上面说过，就是数据包从 Ring Buffer 出来后到上层网络栈的入口。</p>
<p>可以通过 ethtool 查看和设置 GRO：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">查看 GRO</div><div class="line">ethtool -k eth0 | grep generic-receive-offload</div><div class="line">generic-receive-offload: on</div><div class="line">设置开启 GRO</div><div class="line">ethtool -K eth0 gro on</div></pre></td></tr></table></figure>
<h2 id="多-CPU-下的-Ring-Buffer-处理-Receive-Side-Scaling"><a href="#多-CPU-下的-Ring-Buffer-处理-Receive-Side-Scaling" class="headerlink" title="多 CPU 下的 Ring Buffer 处理 (Receive Side Scaling)"></a>多 CPU 下的 Ring Buffer 处理 (Receive Side Scaling)</h2><p>NIC 收到数据的时候产生的 IRQ 只可能被一个 CPU 处理，从而只有一个 CPU 会执行 napi_schedule 来触发 softirq，触发的这个 softirq 的 handler 也还是会在这个产生 softIRQ 的 CPU 上执行。所以 driver 的 <code>poll</code> 函数也是在最开始处理 NIC 发出 IRQ 的那个 CPU 上执行。于是一个 Ring Buffer 上同一个时刻只有一个 CPU 在拉取数据。</p>
<p>从上面描述能看出来分配给 Ring Buffer 的空间是有限的，当收到的数据包速率大于单个 CPU 处理速度的时候 Ring Buffer 可能被占满，占满之后再来的新数据包会被自动丢弃。而现在机器都是有多个 CPU，同时只有一个 CPU 去处理 Ring Buffer 数据会很低效，这个时候就产生了叫做 Receive Side Scaling(RSS) 或者叫做 multiqueue 的机制来处理这个问题。WIKI 对 RSS 的介绍挺好的，简洁干练可以看看: <a href="https://en.wikipedia.org/wiki/Network_interface_controller#RSS" target="_blank" rel="external">Network interface controller - Wikipedia</a></p>
<p>简单说就是现在支持 RSS 的网卡内部会有多个 Ring Buffer，NIC 收到 Frame 的时候能通过 Hash Function 来决定 Frame 该放在哪个 Ring Buffer 上，触发的 IRQ 也可以通过操作系统或者手动配置 IRQ affinity 将 IRQ 分配到多个 CPU 上。这样 IRQ 能被不同的 CPU 处理，从而做到 Ring Buffer 上的数据也能被不同的 CPU 处理，从而提高数据的并行处理能力。</p>
<p>RSS 除了会影响到 NIC 将 IRQ 发到哪个 CPU 之外，不会影响别的逻辑了。收消息过程跟之前描述的是一样的。</p>
<p>如果支持 RSS 的话，NIC 会为每个队列分配一个 IRQ，通过 <code>/proc/interrupts</code> 能进行查看。你可以通过配置 IRQ affinity 指定 IRQ 由哪个 CPU 来处理中断。先通过 <code>/proc/interrupts</code> 找到 IRQ 号之后，将希望绑定的 CPU 号写入 <code>/proc/irq/IRQ_NUMBER/smp_affinity</code>，写入的是 16 进制的 bit mask。比如看到队列 rx_0 对应的中断号是 41 那就执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">echo 6 &gt; /proc/irq/41/smp_affinity</div><div class="line">6 表示的是 CPU2 和 CPU1</div></pre></td></tr></table></figure>
<p>0 号 CPU 的掩码是 0x1 (0001)，1 号 CPU 掩码是 0x2 (0010)，2 号 CPU 掩码是 0x4 (0100)，3 号 CPU 掩码是 0x8 (1000) 依此类推。</p>
<p>另外需要注意的是设置 smp_affinity 的话不能开启 irqbalance 或者需要为 irqbalance 设置 –banirq 列表，将设置了 smp_affinity 的 IRQ 排除。不然 irqbalance 机制运作时会忽略你设置的 IRQ affinity 配置。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L99-L222" target="_blank" rel="external">Receive Packet Steering(RPS)</a> 是在 NIC 不支持 RSS 时候在软件中实现 RSS 类似功能的机制。其好处就是对 NIC 没有要求，任何 NIC 都能支持 RPS，但缺点是 NIC 收到数据后 DMA 将数据存入的还是一个 Ring Buffer，NIC 触发 IRQ 还是发到一个 CPU，还是由这一个 CPU 调用 driver 的 <code>poll</code> 来将 Ring Buffer 的数据取出来。RPS 是在单个 CPU 将数据从 Ring Buffer 取出来之后才开始起作用，它会为每个 Packet 计算 Hash 之后将 Packet 发到对应 CPU 的 backlog 中，并通过 Inter-processor Interrupt(IPI) 告知目标 CPU 来处理 backlog。后续 Packet 的处理流程就由这个目标 CPU 来完成。从而实现将负载分到多个 CPU 的目的。</p>
<p>RPS 默认是关闭的，当机器有多个 CPU 并且通过 softirqs 的统计 <code>/proc/softirqs</code> 发现 NET_RX 在 CPU 上分布不均匀或者发现网卡不支持 mutiqueue 时，就可以考虑开启  RPS。开启 RPS 需要调整 <code>/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus</code> 的值。比如执行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo f &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</div></pre></td></tr></table></figure>
<p>表示的含义是处理网卡 eth0 的 rx-0 队列的 CPU 数设置为 f 。即设置有 15 个 CPU 来处理 rx-0 这个队列的数据，如果你的 CPU 数没有这么多就会默认使用所有 CPU 。甚至有人为了方便都是直接将 <code>echo fff &gt; /sys/class/net/eth0/queues/rx-0/rps_cpus</code> 写到脚本里，这样基本能覆盖所有类型的机器，不管机器 CPU 数有多少，都能覆盖到。从而就能让这个脚本在任意机器都能执行。</p>
<p><strong>注意：</strong>如果 NIC 不支持 mutiqueue，RPS 不是完全不用思考就能打开的，因为其开启之后会加重所有 CPU 的负担，在一些场景下比如 CPU 密集型应用上并不一定能带来好处。所以得测试一下。</p>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L225" target="_blank" rel="external">Receive Flow Steering(RFS)</a> 一般和 RPS 配合一起工作。RPS 是将收到的 packet 发配到不同的 CPU 以实现负载均衡，但是可能同一个 Flow 的数据包正在被 CPU1 处理，但下一个数据包被发到 CPU2，会降低 CPU cache hit 比率并且会让数据包要从 CPU1 发到 CPU2 上。RFS 就是保证同一个 flow 的 packet 都会被路由到正在处理当前 Flow 数据的 CPU，从而提高 CPU cache 比率。<a href="https://github.com/torvalds/linux/blob/master/Documentation/networking/scaling.txt" target="_blank" rel="external">这篇文章</a> 把 RFS 机制介绍的挺好的。基本上就是收到数据后根据数据的一些信息做个 Hash 在这个 table 的 entry 中找到当前正在处理这个 flow 的 CPU 信息，从而将数据发给这个正在处理该 Flow 数据的 CPU 上，从而做到提高 CPU cache hit 率，避免数据在不同 CPU 之间拷贝。当然还有很多细节，请看上面链接。</p>
<p>RFS 默认是关闭的，必须主动配置才能生效。正常来说开启了 RPS 都要再开启 RFS，以获取更好的性能。<a href="https://github.com/torvalds/linux/blob/master/Documentation/networking/scaling.txt" target="_blank" rel="external">这篇文章</a>也有说该怎么去开启 RFS 以及推荐的配置值。一个是要配置 rps_sock_flow_entries</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sysctl -w net.core.rps_sock_flow_entries=32768</div></pre></td></tr></table></figure>
<p>这个值依赖于系统期望的活跃连接数，注意是同一时间活跃的连接数，这个连接数正常来说会大大小于系统能承载的最大连接数，因为大部分连接不会同时活跃。该值建议是 32768，能覆盖大多数情况，每个活跃连接会分配一个 entry。除了这个之外还要配置 rps_flow_cnt，这个值是每个队列负责的 flow 最大数量，如果只有一个队列，则 rps_flow_cnt 一般是跟 rps_sock_flow_entries 的值一致，但是有多个队列的时候 rps_flow_cnt 值就是 rps_sock_flow_entries / N, N 是队列数量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo 2048 &gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt</div></pre></td></tr></table></figure>
<p><a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/networking/scaling.txt#L324" target="_blank" rel="external">Accelerated Receive Flow Steering (aRFS)</a> 类似 RFS 只是由硬件协助完成这个工作。aRFS 对于 RFS 就和 RSS 对于 RPS 一样，就是把 CPU 的工作挪到了硬件来做，从而不用浪费 CPU 时间，直接由 NIC 完成 Hash 值计算并将数据发到目标 CPU，所以快一点。NIC 必须暴露出来一个 <code>ndo_rx_flow_steer</code> 的函数用来实现 aRFS。</p>
<h2 id="adaptive-RX-TX-IRQ-coalescing"><a href="#adaptive-RX-TX-IRQ-coalescing" class="headerlink" title="adaptive RX/TX IRQ coalescing"></a>adaptive RX/TX IRQ coalescing</h2><p>有的 NIC 支持这个功能，用来动态的将 IRQ 进行合并，以做到在数据包少的时候减少数据包的延迟，在数据包多的时候提高吞吐量。查看方法:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ethtool -c eth1</div><div class="line">Coalesce parameters for eth1:</div><div class="line">Adaptive RX: off  TX: off</div><div class="line">stats-block-usecs: 0</div><div class="line">.....</div></pre></td></tr></table></figure>
<p>开启 RX 队列的 adaptive coalescing 执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -C eth0 adaptive-rx on</div></pre></td></tr></table></figure>
<p>并且有四个值需要设置：rx-usecs、rx-frames、rx-usecs-irq、rx-frames-irq，具体含义等需要用到的时候查吧。</p>
<h2 id="Ring-Buffer-相关监控及配置"><a href="#Ring-Buffer-相关监控及配置" class="headerlink" title="Ring Buffer 相关监控及配置"></a>Ring Buffer 相关监控及配置</h2><h3 id="收到数据包统计"><a href="#收到数据包统计" class="headerlink" title="收到数据包统计"></a>收到数据包统计</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">ethtool -S eh0</div><div class="line">NIC statistics:</div><div class="line">     rx_packets: 792819304215</div><div class="line">     tx_packets: 778772164692</div><div class="line">     rx_bytes: 172322607593396</div><div class="line">     tx_bytes: 201132602650411</div><div class="line">     rx_broadcast: 15118616</div><div class="line">     tx_broadcast: 2755615</div><div class="line">     rx_multicast: 0</div><div class="line">     tx_multicast: 10</div></pre></td></tr></table></figure>
<p>RX 就是收到数据，TX 是发出数据。还会展示 NIC 每个队列收发消息情况。<strong>其中比较关键的是带有 drop 字样的统计和 fifo_errors 的统计</strong> :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">tx_dropped: 0</div><div class="line">rx_queue_0_drops: 93</div><div class="line">rx_queue_1_drops: 874</div><div class="line">....</div><div class="line">rx_fifo_errors: 2142</div><div class="line">tx_fifo_errors: 0</div></pre></td></tr></table></figure>
<p>看到发送队列和接收队列 drop 的数据包数量显示在这里。并且所有 queue_drops 加起来等于 rx_fifo_errors。所以总体上能通过 rx_fifo_errors 看到 Ring Buffer 上是否有丢包。如果有的话一方面是看是否需要调整一下每个队列数据的分配，或者是否要加大 Ring Buffer 的大小。</p>
<p><code>/proc/net/dev</code>是另一个数据包相关统计，不过这个统计比较难看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">cat /proc/net/dev</div><div class="line">Inter-|   Receive                                                |  Transmit</div><div class="line"> face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed</div><div class="line">    lo: 14472296365706 10519818839    0    0    0     0          0         0 14472296365706 10519818839    0    0    0     0       0          0</div><div class="line">  eth1: 164650683906345 785024598362    0    0 2142     0          0         0 183711288087530 704887351967    0    0    0     0       0          0</div></pre></td></tr></table></figure>
<h4 id="调整-Ring-Buffer-队列数量"><a href="#调整-Ring-Buffer-队列数量" class="headerlink" title="调整 Ring Buffer 队列数量"></a>调整 Ring Buffer 队列数量</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ethtool -l eth0</div><div class="line">Channel parameters for eth0:</div><div class="line">Pre-set maximums:</div><div class="line">RX:             0</div><div class="line">TX:             0</div><div class="line">Other:          1</div><div class="line">Combined:       8</div><div class="line">Current hardware settings:</div><div class="line">RX:             0</div><div class="line">TX:             0</div><div class="line">Other:          1</div><div class="line">Combined:       8</div></pre></td></tr></table></figure>
<p>看的是 Combined 这一栏是队列数量。Combined 按说明写的是多功能队列，猜想是能用作 RX 队列也能当做 TX 队列，但数量一共是 8 个？</p>
<p>如果不支持 mutiqueue 的话上面执行下来会是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Channel parameters for eth0:</div><div class="line">Cannot get device channel parameters</div><div class="line">: Operation not supported</div></pre></td></tr></table></figure>
<p>看到上面 Ring Buffer 数量有 maximums 和 current settings，所以能自己设置 Ring Buffer 数量，但最大不能超过 maximus 值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ethtool -L eth0 combined 8</div></pre></td></tr></table></figure>
<p>如果支持对特定类型 RX 或 TX 设置队列数量的话可以执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ethtool -L eth0 rx 8</div></pre></td></tr></table></figure>
<p><strong>需要注意的是</strong>，ethtool 的设置操作可能都要重启一下才能生效。</p>
<h4 id="调整-Ring-Buffer-队列大小"><a href="#调整-Ring-Buffer-队列大小" class="headerlink" title="调整 Ring Buffer 队列大小"></a>调整 Ring Buffer 队列大小</h4><p>先查看当前 Ring Buffer 大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">ethtool -g eth0</div><div class="line">Ring parameters for eth0:</div><div class="line">Pre-set maximums:</div><div class="line">RX:   4096</div><div class="line">RX Mini:  0</div><div class="line">RX Jumbo: 0</div><div class="line">TX:   4096</div><div class="line">Current hardware settings:</div><div class="line">RX:   512</div><div class="line">RX Mini:  0</div><div class="line">RX Jumbo: 0</div><div class="line">TX:   512</div></pre></td></tr></table></figure>
<p>看到 RX 和 TX 最大是 4096，当前值为 512。<strong>队列越大丢包的可能越小，但数据延迟会增加</strong></p>
<p>设置 RX 队列大小：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -G eth0 rx 4096</div></pre></td></tr></table></figure>
<h4 id="调整-Ring-Buffer-队列的权重"><a href="#调整-Ring-Buffer-队列的权重" class="headerlink" title="调整 Ring Buffer 队列的权重"></a>调整 Ring Buffer 队列的权重</h4><p>NIC 如果支持 mutiqueue 的话 NIC 会根据一个 Hash 函数对收到的数据包进行分发。能调整不同队列的权重，用于分配数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">ethtool -x eth0</div><div class="line">RX flow hash indirection table for eth0 with 8 RX ring(s):</div><div class="line">    0:      0     0     0     0     0     0     0     0</div><div class="line">    8:      0     0     0     0     0     0     0     0</div><div class="line">   16:      1     1     1     1     1     1     1     1</div><div class="line">   ......</div><div class="line">   64:      4     4     4     4     4     4     4     4</div><div class="line">   72:      4     4     4     4     4     4     4     4</div><div class="line">   80:      5     5     5     5     5     5     5     5</div><div class="line">   ......</div><div class="line">  120:      7     7     7     7     7     7     7     7</div></pre></td></tr></table></figure>
<p>我的 NIC 一共有 8 个队列，一个有 128 个不同的 Hash 值，上面就是列出了每个 Hash 值对应的队列是什么。最左侧 0 8 16 是为了能让你快速的找到某个具体的 Hash 值。比如 Hash 值是 76 的话我们能立即找到 72 那一行：”72:      4     4     4     4     4     4     4     4”，从左到右第一个是 72 数第 5 个就是 76 这个 Hash 值对应的队列是 4 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -X eth0 weight 6 2 8 5 10 7 1 5</div></pre></td></tr></table></figure>
<p>设置 8 个队列的权重。加起来不能超过 128 。128 是 indirection table 大小，每个 NIC 可能不一样。</p>
<h4 id="更改-Ring-Buffer-Hash-Field"><a href="#更改-Ring-Buffer-Hash-Field" class="headerlink" title="更改 Ring Buffer Hash Field"></a>更改 Ring Buffer Hash Field</h4><p>分配数据包的时候是按照数据包内的某个字段来进行的，这个字段能进行调整。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ethtool -n eth0 rx-flow-hash tcp4</div><div class="line">TCP over IPV4 flows use these fields for computing Hash flow key:</div><div class="line">IP SA</div><div class="line">IP DA</div><div class="line">L4 bytes 0 &amp; 1 [TCP/UDP src port]</div><div class="line">L4 bytes 2 &amp; 3 [TCP/UDP dst port]</div></pre></td></tr></table></figure>
<p>查看 tcp4 的 Hash 字段。</p>
<p>也可以设置 Hash 字段：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ethtool -N eth0 rx-flow-hash udp4 sdfn</div></pre></td></tr></table></figure>
<p>sdfn 需要查看 ethtool 看其含义，还有很多别的配置值。</p>
<h4 id="softirq-数统计"><a href="#softirq-数统计" class="headerlink" title="softirq 数统计"></a>softirq 数统计</h4><p>通过 <code>/proc/softirqs</code> 能看到每个 CPU 上 softirq 数量统计：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">cat /proc/softirqs</div><div class="line">                    CPU0       CPU1       </div><div class="line">          HI:          1          0</div><div class="line">       TIMER: 1650579324 3521734270</div><div class="line">      NET_TX:   10282064   10655064</div><div class="line">      NET_RX: 3618725935       2446</div><div class="line">       BLOCK:          0          0</div><div class="line">BLOCK_IOPOLL:          0          0</div><div class="line">     TASKLET:      47013      41496</div><div class="line">       SCHED: 1706483540 1003457088</div><div class="line">     HRTIMER:    1698047   11604871</div><div class="line">         RCU: 4218377992 3049934909</div></pre></td></tr></table></figure>
<p>看到 NET_RX 就是收消息时候触发的 softirq，一般看这个统计是为了看看 softirq 在每个 CPU 上分布是否均匀，不均匀的话可能就需要做一些调整。比如上面看到 CPU0 和 CPU1 两个差距很大，原因是这个机器的 NIC 不支持 RSS，没有多个 Ring Buffer。开启 RPS 后就均匀多了。</p>
<h4 id="IRQ-统计"><a href="#IRQ-统计" class="headerlink" title="IRQ 统计"></a>IRQ 统计</h4><p><code>/proc/interrupts</code> 能看到每个 CPU 的 IRQ 统计。一般就是看看 NIC 有没有支持 multiqueue 以及 NAPI 的 IRQ 合并机制是否生效。看看 IRQ 是不是增长的很快。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/" data-id="cjagai4uu0007cowxlxcfk8mo" class="article-share-link">Share</a><div class="tags"><a href="/tags/Network/">Network</a></div><div class="post-nav"><a href="/2017/07/24/linux-receive-packet-2/" class="pre">Linux 网络协议栈收消息过程-Per CPU Backlog</a><a href="/2017/07/02/RTM-max-connections/" class="next">实时通信系统并发连接数测试时需要调整的各种参数</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/Network/" style="font-size: 15px;">Network</a> <a href="/tags/TCP/" style="font-size: 15px;">TCP</a> <a href="/tags/Bug/" style="font-size: 15px;">Bug</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/Others/" style="font-size: 15px;">Others</a> <a href="/tags/Netty/" style="font-size: 15px;">Netty</a> <a href="/tags/Algorithm/" style="font-size: 15px;">Algorithm</a> <a href="/tags/Clojure/" style="font-size: 15px;">Clojure</a> <a href="/tags/JVM/" style="font-size: 15px;">JVM</a> <a href="/tags/Cache/" style="font-size: 15px;">Cache</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/01/15/cache-structure/">CPU Cache</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/11/11/netty-resource-leack-detector/">Netty 的资源泄露探测机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/25/transmittable-thread-local/">线程之间传递 ThreadLocal 对象</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/09/21/java-threadlocal/">Java ThreadLocal</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/12/idea-import-gradle-project/">Intellij IDEA 导入 Gradle 项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/01/linux-receive-packet-3/">Linux 网络协议栈收消息过程-TCP Protocol Layer</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/24/linux-receive-packet-2/">Linux 网络协议栈收消息过程-Per CPU Backlog</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/23/linux-receive-packet-1/">Linux 网络协议栈收消息过程-Ring Buffer</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/02/RTM-max-connections/">实时通信系统并发连接数测试时需要调整的各种参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/tcp-time-wait/">TCP TIME-WAIT</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">A Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>